{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl/56muGqgfrV+BxmuS2Q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhakaran-s-code/genai-python/blob/main/Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "This notebook contains the code to perform semantic search on the data extracted from a set of webpages and return the most relevant URLs. The logic for semantic search is done based on Bert bi-encoder and cross-encoder models. Initially semantic search is done using bi-encoder model and re-ranking is done using a cross-encoder model to get better results."
      ],
      "metadata": {
        "id": "lcfmGV1-P7O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "X6OgPCdSqpP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "gkITM8DpbDbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ea2f9f-b874-41e7-95b4-f99c88bec9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all the urls from sitemap.xml for a website\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_urls_from_sitemap(sitemap_url):\n",
        "    response = requests.get(sitemap_url)\n",
        "    soup = BeautifulSoup(response.text, 'xml')\n",
        "\n",
        "    urls = []\n",
        "    for link in soup.find_all('loc'):\n",
        "          urls.append(link.text)\n",
        "\n",
        "    return urls\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sitemap_url = 'https://www.abc.com/sitemap.xml'\n",
        "    urls = extract_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    for url in urls:\n",
        "        print(url)\n"
      ],
      "metadata": {
        "id": "IlIT-2FBzQwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract content from URL\n",
        "def extract_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
        "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
        "    meta = []\n",
        "    for meta_tag in soup.find_all('meta', {'name': ['title', 'description', 'path', 'tags']}):\n",
        "        if meta_tag.get('content') is not None:\n",
        "            meta.append(meta_tag.get('content'))\n",
        "    return ' '.join(meta + headers + paragraphs)"
      ],
      "metadata": {
        "id": "MKmRsIV8bH-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
        "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
        "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
        "\n",
        "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# Extract content from all URLs\n",
        "documents = [extract_content(url) for url in urls]\n",
        "\n",
        "\n",
        "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
        "corpus_embeddings = bi_encoder.encode(documents, convert_to_tensor=True, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "aA9RjvtqfLSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### Semantic Search #####\n",
        "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
        "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
        "    hits = hits[0]  # Get the hits for the first query\n",
        "\n",
        "    ##### Re-Ranking #####\n",
        "    # Now, score all retrieved passages with the cross_encoder\n",
        "    cross_inp = [[query, documents[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "\n",
        "    # Sort results by the cross-encoder scores\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx]['cross-score'] = cross_scores[idx]\n",
        "\n",
        "    # Output of top-5 hits from bi-encoder\n",
        "    print(\"\\n-------------------------\\n\")\n",
        "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "    for hit in hits[0:10]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], urls[hit['corpus_id']]))\n",
        "\n",
        "    # Output of top-5 hits from re-ranker\n",
        "    print(\"\\n-------------------------\\n\")\n",
        "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    for hit in hits[0:10]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], urls[hit['corpus_id']]))\n"
      ],
      "metadata": {
        "id": "mA3EklNigPw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search(query = \"# user query #\")"
      ],
      "metadata": {
        "id": "X9_eFhL6jdy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below commented code performs semantic search using cosine similarity which was initially tried."
      ],
      "metadata": {
        "id": "MSy3FtcgR826"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of URLs for content extraction\n",
        "#urls = [\"https://www.abc.com/\", \"https://www.abc.com/about-us\"]"
      ],
      "metadata": {
        "id": "TxSHSofCbFtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell and next one uses Sentence Transformer to embed the documents/query and perform semantic search\n",
        "# Extract content from all URLs\n",
        "#documents = [extract_content(url) for url in urls]\n",
        "\n",
        "# Load pre-trained Sentence Transformer model\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for documents\n",
        "#document_embeddings = model.encode(documents)"
      ],
      "metadata": {
        "id": "55x4NyLJbLzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a semantic search\n",
        "#query = 'tell me something about AI WisdomNext'\n",
        "#query_embedding = model.encode([query])\n",
        "\n",
        "# Compute cosine similarity between query and document embeddings\n",
        "#cosine_scores = cosine_similarity(query_embedding, document_embeddings)\n",
        "\n",
        "#best_match = cosine_scores.argmax()\n",
        "\n",
        "# Print the document and its similarity score\n",
        "#for document_number, score in sorted(enumerate(cosine_scores[0]), key=lambda x: x[1], reverse=True):\n",
        "#    print(document_number, score)\n",
        "#print(urls[best_match])"
      ],
      "metadata": {
        "id": "DnKfZpzupRqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}