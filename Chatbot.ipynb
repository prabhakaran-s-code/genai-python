{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhakaran-s-code/genai-python/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "This notebook contains the code to build a contextual chatbot. A set of web pages containing contextual information is fed in the input, relevant information from the webpages extracted and stored in embedded format using fiass library. When the user asks a query, the same is embedded, semantic search performed in the faiss index to get the best matched context. The query along with the context is passed on to a generative AI model to generate the response. Couple of Gen AI models like Databricks DollyV2 and gpt4all falcon were tried. Some basic grounding logic is also implemented to ensure the generated ouput is relevant."
      ],
      "metadata": {
        "id": "OZM2C0uDL4rQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfApNmDqCRUH"
      },
      "outputs": [],
      "source": [
        "#!pip install gpt4all --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install faiss-cpu --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip install torch transformers accelerate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIoPRcr96l8Q"
      },
      "outputs": [],
      "source": [
        "#!wget -P path/to/models https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmUhyzS_yzE4"
      },
      "outputs": [],
      "source": [
        "import requests, re, nltk, torch, faiss, sqlite3, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline,  AutoTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "#from gpt4all import GPT4All\n",
        "#model = GPT4All(\"/content/path/to/models/gpt4all-falcon-newbpe-q4_0.gguf\")\n",
        "\n",
        "# Load the model (replace with the specific Dolly model you choose)\n",
        "generator = pipeline(model=\"databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n",
        "# Load the tokenizer associated with the Dolly model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RxGkAu8co0p"
      },
      "outputs": [],
      "source": [
        "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
        "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
        "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB8iZ19IAxNx"
      },
      "outputs": [],
      "source": [
        "# Extract all the urls from sitemap.xml for a website\n",
        "\n",
        "def extract_urls_from_sitemap(sitemap_url):\n",
        "    response = requests.get(sitemap_url)\n",
        "    soup = BeautifulSoup(response.text, 'xml')\n",
        "\n",
        "    for link in soup.find_all('loc'):\n",
        "          urls.append(link.text)\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCRnfFXTA02q"
      },
      "outputs": [],
      "source": [
        "# Function to extract content from URL\n",
        "def extract_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
        "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
        "    meta = []\n",
        "    for meta_tag in soup.find_all('meta', {'name': ['title', 'description', 'path', 'tags']}):\n",
        "        if meta_tag.get('content') is not None:\n",
        "            meta.append(' ' + meta_tag.get('content'))\n",
        "    # Remove unwanted characters (customize as needed)\n",
        "    return (' '.join(meta +  headers + paragraphs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgSAO2Xa6X2D"
      },
      "outputs": [],
      "source": [
        "def encode_content(urls):\n",
        "    data = {'sentence': [], 'url': []}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    for url in urls:\n",
        "        id = 0\n",
        "        page_content = extract_content(url)\n",
        "\n",
        "        temp_sentences = nltk.sent_tokenize(page_content)\n",
        "        for sentence in temp_sentences:\n",
        "            sentence = re.sub(r'[^a-zA-Z0-9\\.,!?/\\-’™():% ]', ' ', sentence) # clean-up invalid characters in the sentence\n",
        "#            temp_sentences[index] = sentence\n",
        "            # Instead of appending to the DataFrame directly, create a temporary DataFrame and concatenate\n",
        "            temp_df = pd.DataFrame({'sentence': [sentence], 'url': [url]})\n",
        "            df = pd.concat([df, temp_df], ignore_index=True)\n",
        "\n",
        "    write_df_to_db(df)\n",
        "\n",
        "    embeddings =bi_encoder.encode(df['sentence'].tolist(), show_progress_bar=True)\n",
        "    create_faiss_index(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzLAfTbkSEZA"
      },
      "outputs": [],
      "source": [
        "def write_df_to_db(df):\n",
        "    # Connect to database (create if it doesn't exist)\n",
        "    conn = sqlite3.connect('sentences_db.sqlite')\n",
        "    cursor = conn.cursor()\n",
        "    # Create table (if it doesn't exist)\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS sentences (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "            sentence TEXT,\n",
        "            url TEXT\n",
        "        )\n",
        "    ''')\n",
        "\n",
        "    # Insert data from DataFrame\n",
        "    df.to_sql('sentences', conn, if_exists='replace', index_label='id')\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7CDp5NnRzY3"
      },
      "outputs": [],
      "source": [
        "def create_faiss_index(embeddings):\n",
        "    # Assuming 'sentence_encodings' is a numpy array of shape (num_sentences, embedding_dim)\n",
        "    dim = embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatIP(dim)  # Create a flat index for exact search\n",
        "    faiss_index.add(embeddings)  # Add the sentence encodings to the index\n",
        "    faiss.write_index(faiss_index, \"my_index.faiss\")  # Save index to disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okpydZNE6ng8"
      },
      "outputs": [],
      "source": [
        "def search(query):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### Semantic Search #####\n",
        "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
        "    query_embedding = bi_encoder.encode(query)\n",
        "\n",
        "    # Load Faiss index (assuming it's already created and saved)\n",
        "    index = faiss.read_index(\"my_index.faiss\")\n",
        "    distances, indices = index.search(query_embedding.reshape(1, -1), k=5)\n",
        "\n",
        "    # Connect to SQLite database\n",
        "    conn = sqlite3.connect('sentences_db.sqlite')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Retrieve sentences from SQLite\n",
        "    retrieved_sentences = []\n",
        "    for i in indices[0]:\n",
        "        cursor.execute(\"SELECT sentence FROM sentences WHERE id = \" + str(i))\n",
        "        result = cursor.fetchone()\n",
        "        if result:\n",
        "            retrieved_sentences.append(result[0])\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    # Print results\n",
        "    context =\" \".join(retrieved_sentences)\n",
        "\n",
        "    # Return the first retrieved sentence, or None if no sentences were found\n",
        "    if context:\n",
        "        return context\n",
        "    else:\n",
        "        return None  # Handle the case when no sentences are found"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect('sentences_db.sqlite')\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"drop table sentences\")\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "uWEkVdJLL3oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3Lm_cnAdtYo"
      },
      "outputs": [],
      "source": [
        "urls = []\n",
        "sentences = []\n",
        "sentences_embedding = []\n",
        "encode_content(extract_urls_from_sitemap('https://www.abc.com/sitemap.xml'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gbr8RU45eg_"
      },
      "outputs": [],
      "source": [
        "# This code uses gpt4all model. Hence commented.\n",
        "#query = '#Some user query#'\n",
        "#context = search(query)\n",
        "\n",
        "#system_template = 'System: You are a contextual chat bot, you will be presented a context from which the a question will be asked, give your valuable insights as well. If you cannot find the answer to the user\\'s query in the context, politely respond the same to the user.\\nContext: '+ context + '\\n'\n",
        "#prompt_template = 'Query: {0}. \\n Response: '\n",
        "#with model.chat_session(system_template, prompt_template):\n",
        "#    response = model.generate(query, max_tokens=100, temp=0.0, top_k=30, top_p=0.0, min_p=0.0, repeat_penalty=10, repeat_last_n=64, n_batch=1, n_predict=None, streaming=False)\n",
        "#    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_keywords(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    keywords = [word.lower() for word, tag in tagged_tokens if tag.startswith('N') or tag.startswith('V') or tag.startswith('J')]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_keywords = [word for word in keywords if word.lower() not in stop_words]\n",
        "    return set(filtered_keywords)\n",
        "\n"
      ],
      "metadata": {
        "id": "tvzGzl38l9_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_KBpsmpQd7q"
      },
      "outputs": [],
      "source": [
        "# Define your context and system prompt\n",
        "query = \"# user query #\"\n",
        "context = search(query)\n",
        "\n",
        "system_prompt = \"You are a contextual chat bot, you will be presented a context from which the a question will be asked. Please respond to the user precisely based on the context. Cite specific passages from the context to support your claims in the response. Add a reference URL in the response if an URL is provided in the context. If you cannot find the answer to the user\\'s query in the context, politely respond the same to the user.\"\n",
        "# Construct the prompt with context and system prompt\n",
        "prompt = f\"\"\"{system_prompt}\n",
        "Context: {context}\n",
        "User: {query}\n",
        "Assistant: \"\"\"  # Simulate a conversation-like structure\n",
        "\n",
        "# Tokenize the user text.\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "generate_kwargs = dict(\n",
        "    model_inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_p=1.0,\n",
        "    temperature=float(0.0),\n",
        "    repetition_penalty= 2.0\n",
        ")\n",
        "\n",
        "# Pass input_ids and attention_mask through generation_config\n",
        "res = generator(prompt, kwargs=generate_kwargs)\n",
        "\n",
        "\n",
        "context_keywords = extract_keywords(context)\n",
        "response_keywords = extract_keywords(res[0]['generated_text'])\n",
        "\n",
        "if response_keywords.issubset(context_keywords):\n",
        "    print(\"All valid keywords in the response are present in the context. /n\")\n",
        "    print(res[0]['generated_text'])\n",
        "else:\n",
        "    missing_keywords = response_keywords - context_keywords\n",
        "    num_missing_keywords = len(missing_keywords)\n",
        "    print(\"The following keywords are in the response but not in the context:\", missing_keywords)\n",
        "\n",
        "    # Regenerate the response if there are more than 3 missing keywords between the response and context\n",
        "    if num_missing_keywords > 3:\n",
        "        print('--Regenerating Response--')\n",
        "        res = generator(prompt, kwargs=generate_kwargs)\n",
        "        context_keywords = extract_keywords(context)\n",
        "        response_keywords = extract_keywords(res[0]['generated_text'])\n",
        "\n",
        "    # Check similarity score of the context and query\n",
        "\n",
        "    context_embedding = bi_encoder.encode(context)\n",
        "    response_embedding = bi_encoder.encode(res[0]['generated_text'])\n",
        "\n",
        "    similarity_score = util.pytorch_cos_sim(context_embedding, response_embedding)\n",
        "    print(\"similarity_score: \", similarity_score)\n",
        "\n",
        "    if(similarity_score > 0.75):\n",
        "        print(res[0]['generated_text'])\n",
        "    else:\n",
        "        print('Apologies! I cannot answer this query. Please visit https://www.abc.com and post your query in the Contact Us section. If the query is relevant, one of our associates will get back to you shortly.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUQsQl6Is30A"
      },
      "outputs": [],
      "source": [
        "print(context)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_keywords(context))\n",
        "print(extract_keywords(res[0]['generated_text']))"
      ],
      "metadata": {
        "id": "Bj7vEa7Q5bP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMi11XkugBSfvkF20rPVaJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}