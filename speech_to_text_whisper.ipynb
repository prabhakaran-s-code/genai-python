{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLuzq+9FMFTLqUZXesyQjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhakaran-s-code/genai-python/blob/main/speech_to_text_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "This is a python code to convert speech to text for a wide variety of video and audio files including speaker diarization. The transcript files will be generated in txt format and captions in vtt format. Open AI Whisper Model is used for generating caption/transcript and Nvidia NeMo model is used for speaker diarization"
      ],
      "metadata": {
        "id": "d9TAdlyC_Lup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Whisper\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "# Install NeMo\n",
        "BRANCH = 'r1.23.0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "#!pip install text-unidecode\n",
        "!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html # Install TorchAudio\n",
        "\n",
        "import json, datetime, json, subprocess, whisper, os, re\n",
        "from nemo.collections.asr.models import ClusteringDiarizer\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "eGkK_LRYhZmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the intermediate/final output folder paths\n",
        "vttpath = \"path/to/subtitles/\"\n",
        "txtpath = \"path/to/transcripts/\"\n",
        "json_path = \"path/to/json/\"\n",
        "diarization_path = \"path/to/diarization/\"\n",
        "video_path = \"path/to/videos/\"\n",
        "\n",
        "if not os.path.exists(vttpath):\n",
        "  os.makedirs(vttpath)\n",
        "\n",
        "if not os.path.exists(txtpath):\n",
        "  os.makedirs(txtpath)\n",
        "\n",
        "if not os.path.exists(json_path):\n",
        "  os.makedirs(json_path)\n",
        "\n",
        "if not os.path.exists(video_path):\n",
        "  os.makedirs(video_path)\n",
        "\n",
        "if not os.path.exists(diarization_path):\n",
        "  os.makedirs(diarization_path)\n",
        "\n",
        "# meta data for NeMo model\n",
        "meta = {\n",
        "    'audio_filepath': '',\n",
        "    'offset': 0,\n",
        "    'duration':None,\n",
        "    'label': 'infer',\n",
        "    'text': '-',\n",
        "    'num_speakers': 2,\n",
        "    'rttm_filepath': None,\n",
        "    'uem_filepath' : None\n",
        "}\n",
        "\n",
        "# Load Model Config for NeMo model\n",
        "MODEL_CONFIG = os.path.join(diarization_path,'diar_infer_telephonic.yaml')\n",
        "if not os.path.exists(MODEL_CONFIG):\n",
        "    !wget -P $diarization_path \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_telephonic.yaml\"\n",
        "\n",
        "config = OmegaConf.load(MODEL_CONFIG)\n",
        "#print(OmegaConf.to_yaml(config))\n",
        "config.diarizer.manifest_filepath = diarization_path + 'input_manifest.json'\n",
        "config.diarizer.out_dir = diarization_path #Directory to store intermediate files and prediction outputs"
      ],
      "metadata": {
        "id": "fjR4xqnZizgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize URLs containing media files for which transcript/caption has to be generated\n",
        "urls = [#add the urls of video or audio files separated by comma\n",
        "        ]"
      ],
      "metadata": {
        "id": "--Ev6hSypQ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeGW60q7C2aH"
      },
      "outputs": [],
      "source": [
        "# Function to convert seconds to HH:mm:ss.SSS format\n",
        "def convert_seconds(total_seconds):\n",
        "  \"\"\"\n",
        "  Converts seconds to a string in the format HH:mm:ss.SSS.\n",
        "\n",
        "  Args:\n",
        "    total_seconds: The number of seconds to convert.\n",
        "\n",
        "  Returns:\n",
        "    A string in the format HH:mm:ss.SSS.\n",
        "  \"\"\"\n",
        "  hours = int(total_seconds // 3600)\n",
        "  minutes = int((total_seconds % 3600) // 60)\n",
        "  seconds = int(total_seconds % 60)\n",
        "  milliseconds = int((total_seconds * 1000) % 1000)\n",
        "\n",
        "  return f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write the complete transcribed text to a json, text and vtt file\n",
        "def write_to_file_wh(transcript, filename):\n",
        "  with open(json_path + os.path.splitext(os.path.basename(filename))[0]+\".json\", 'w') as f:\n",
        "    f.write(json.dumps(transcript))\n",
        "\n",
        "  with open(txtpath + os.path.splitext(os.path.basename(filename))[0]+\".txt\", 'w') as f:\n",
        "    text = transcript['text']\n",
        "    f.write(text + '\\n')\n",
        "  captions = transcript['segments']\n",
        "  #print(caption)\n",
        "\n",
        "  with open(vttpath + os.path.splitext(os.path.basename(filename))[0]+\".vtt\", 'w') as f:\n",
        "    f.write('WEBVTT' + '\\n\\n')\n",
        "    captions = transcript['segments']\n",
        "    for output_dict in captions:\n",
        "        text = output_dict['text']\n",
        "        if(text != \"\"):\n",
        "            output_start_time = convert_seconds(output_dict['start'])\n",
        "            output_end_time = convert_seconds(output_dict['end'])\n",
        "            f.write(output_start_time + \" --> \" + output_end_time + '\\n' + text.strip() + '\\n\\n')"
      ],
      "metadata": {
        "id": "LEZUdkgpC5qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_rttm_file(base_video_file_name):\n",
        "    turns = []  # List to store turn information\n",
        "    rttm_path = diarization_path + 'pred_rttms/' + base_video_file_name + '.rttm'\n",
        "    print(rttm_path)\n",
        "    with open(rttm_path, 'r') as rttm_file:\n",
        "        for line in rttm_file:\n",
        "            fields = line.strip().split()  # Split line by spaces\n",
        "            if len(fields) == 10:\n",
        "                # Extract relevant fields\n",
        "                turn_info = {\n",
        "                    'Type': fields[0],\n",
        "                    'File ID': fields[1],\n",
        "                    'Turn Onset': float(fields[3]),\n",
        "                    'Turn Duration': float(fields[4]),\n",
        "                    'Speaker Name': fields[7]\n",
        "                }\n",
        "                turns.append(turn_info)\n",
        "    return turns"
      ],
      "metadata": {
        "id": "X8_78zZRr7Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_speaker_label(turns, start_time, end_time):\n",
        "    # Find the relevant line(s) in the RTTM data\n",
        "    if start_time == 0.00:\n",
        "        return turns[0]['Speaker Name']\n",
        "\n",
        "    for turn_info in turns:\n",
        "#        print(str(turn_info['Turn Onset']) + \" --- \" + str((turn_info['Turn Onset'] + turn_info['Turn Duration'])))\n",
        "        if int(turn_info['Turn Onset']) <= start_time <= (int((turn_info['Turn Onset']) + int(turn_info['Turn Duration']))):\n",
        "            return turn_info['Speaker Name']\n",
        "\n",
        "#        print(str(turn_info['Turn Onset']) + \" --- \" + str((turn_info['Turn Onset'] + turn_info['Turn Duration'])))\n",
        "        if int(turn_info['Turn Onset']) <= end_time <= (int((turn_info['Turn Onset']) + int(turn_info['Turn Duration']))):\n",
        "            return turn_info['Speaker Name']\n",
        "\n",
        "    # If no match found, return a default label (e.g., 'Unknown')\n",
        "    return 'Unknown'"
      ],
      "metadata": {
        "id": "CkAttpDSxUrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transcript_json(json_transcript_path):\n",
        "    with open(json_transcript_path, 'r') as json_file:\n",
        "        return json.load(json_file)"
      ],
      "metadata": {
        "id": "hAZe29gJ7ueR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_speaker_labels(transcript_json, base_video_file_name):\n",
        "  previous_speaker = ''\n",
        "  turns = load_rttm_file(base_video_file_name)\n",
        "  with open(txtpath + base_video_file_name+\"_speaker.txt\", 'w') as f:\n",
        "    for index, segment in enumerate(transcript_json['segments'], start =0):\n",
        "        # Match segment timestamps with RTTM data\n",
        "        # and add the corresponding speaker label\n",
        "        start_time = int(segment['start'])\n",
        "        end_time = int(segment['end'])\n",
        "        speaker = get_speaker_label(turns, start_time, end_time)\n",
        "        if speaker != 'Unknown':\n",
        "            number = int(speaker.split(\"_\")[-1])\n",
        "            number += 1\n",
        "            speaker = \"speaker_\" + str(number)\n",
        "        segment['speaker'] = speaker\n",
        "        # convert time to HH:mm:ss.SSS format and strip off milliseconds\n",
        "        time = re.sub(r'\\.\\d+$', '',convert_seconds(start_time))\n",
        "        if previous_speaker == speaker :\n",
        "            f.write(segment['text'])\n",
        "        else :\n",
        "            # f.write(\" \" + time + \" \" + speaker + '\\n')\n",
        "            if (index == 0):\n",
        "                f.write(speaker + '\\n')\n",
        "            else:\n",
        "                f.write('\\n\\n' + speaker + '\\n')\n",
        "            f.write(segment['text'].strip())\n",
        "        #f.write(time + \" \" + speaker + '\\n')\n",
        "        previous_speaker = speaker\n",
        "    return transcript_json\n",
        "\n",
        "# Example usage\n",
        "#transcript_json_path = '/content/path/to/json/abc.json'\n",
        "\n",
        "#transcript_data = load_transcript_json(transcript_json_path)\n",
        "#updated_transcript = add_speaker_labels(transcript_data)\n",
        "\n",
        "#print(updated_transcript)"
      ],
      "metadata": {
        "id": "TNSvStBpxT7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_file_name(url):\n",
        "  \"\"\"Extracts the file name from the end of a URL.\"\"\"\n",
        "  match = re.search(r'/([^/]+)$', url)\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "lGAqkLntmBn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main block\n",
        "import gc\n",
        "for url in urls:\n",
        "  #print (url)\n",
        "  !wget -P path/to/videos {url}\n",
        "  filename = video_path+extract_file_name(url)\n",
        "  result = model.transcribe(filename) #Transcribe the video file using Whisper Model\n",
        "  #print(result)\n",
        "  write_to_file_wh(result, filename) #Write the output transcript json, text and vtt files\n",
        "  base_video_file_name = os.path.splitext(os.path.basename(filename))[0]\n",
        "  wav_file_path = video_path + base_video_file_name+\".wav\"\n",
        "  subprocess.call(['ffmpeg', '-i', filename,'-ac', '1' , wav_file_path, '-y'])\n",
        "  meta['audio_filepath'] = wav_file_path\n",
        "  with open(diarization_path + 'input_manifest.json','w') as fp:\n",
        "      json.dump(meta,fp)\n",
        "      fp.write('\\n')\n",
        "  sd_model = ClusteringDiarizer(cfg=config)\n",
        "  sd_model.diarize() #Extract speaker information from the audio file\n",
        "\n",
        "  transcript_data = load_transcript_json(json_path + base_video_file_name+\".json\")\n",
        "  add_speaker_labels(transcript_data, base_video_file_name)\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "yKAjXyGzC8Ka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}